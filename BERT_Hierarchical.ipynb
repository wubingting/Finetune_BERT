{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "#import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133dba4",
   "metadata": {},
   "source": [
    "### BERT_Hierarchical\n",
    "\n",
    "The code is to fine-tune BERT by computing the pooled result from the output of each segment of the long document.\n",
    "It first uses BertTokenizer to encode the document, \n",
    "then get the encoding of the first 512 tokens and also the overflowing tokens if the document has more than the defined max length tokens.\n",
    "Based on the encoding of the first 512 tokens and the overflowing tokens (ids), we can get the encoding of the whole document and then segment\n",
    "the encoding with overlapping tokens. The encoding of each segment is then as input to the Bert model.\n",
    "For each segment we will get an output and then do pooling.\n",
    "\n",
    "The code is debugged step by step and some comments are added with small examples.\n",
    "\n",
    "The main code is extracted from https://github.com/helmy-elrais/RoBERT_Recurrence_over_BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ce127",
   "metadata": {},
   "source": [
    "### Data Preparing\n",
    "\n",
    "get the 20newsgroups dataset and then split into training set (90%) and validation set(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ \"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\",]\n",
    "\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, \n",
    "                                        random_state=238, remove=remove)\n",
    "# val_ng = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, \n",
    "#                                        random_state=42, remove=remove)\n",
    "\n",
    "# newsgroups = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "#                                       random_state=238, remove=remove)\n",
    "# test_ng = fetch_20newsgroups(subset='test', shuffle=True, \n",
    "#                                       random_state=238, remove=remove)\n",
    "\n",
    "data_train, data_val, label_train, label_val = train_test_split(newsgroups.data, newsgroups.target, test_size=0.1, random_state=42)\n",
    "train_ng = {'data': data_train, 'target': label_train}\n",
    "val_ng = {'data': data_val, 'target': label_val}\n",
    "\n",
    "print(\"data loaded\")\n",
    "\n",
    "# print('size of training set:', len(train_ng.data))\n",
    "# print('size of validation set:', len(val_ng.data))\n",
    "# print('classes:', train_ng.target_names)\n",
    "print('size of training set:', len(data_train))\n",
    "print('size of validation set:', len(data_val))\n",
    "print('classes:', newsgroups.target_names)\n",
    "\n",
    "# data_train = train_ng.data\n",
    "# label_train = train_ng.target\n",
    "# data_test = test_ng.data\n",
    "# label_test = test_ng.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04121ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#length = [len(l) for l in train_ng.data]\n",
    "length = [len(l) for l in train_ng['data']]\n",
    "plt.hist(length, bins=1000)\n",
    "plt.title(\"Frequency Histogram\")\n",
    "plt.xlabel(\"length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim((0, 5000))\n",
    "plt.xticks(np.arange(0, 5000, step=500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing BertTokenizer')\n",
    "\n",
    "BERTMODEL='bert-base-uncased'\n",
    "CACHE_DIR='transformers-cache'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL, cache_dir=CACHE_DIR,\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4180b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda:2\") # specify  devicethe\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input_ids_list, attention_mask_list of the long document. Each element in the list correspondes to a segment.\n",
    "# also get target for each segment and the number of segments.\n",
    "class LongNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, docs, targets, tokenizer, max_len, chunk_len=512, overlap_len=50):\n",
    "        self.docs = docs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        doc = str(self.docs[item])\n",
    "        target = int(self.targets[item])\n",
    "        \n",
    "        # get the encoding of the first 512 tokens and the overflowing tokens if the document has more than the defined max length tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          doc,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.chunk_len,\n",
    "          truncation=True,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_overflowing_tokens=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        long_token = self.long_terms_tokenizer(encoding, target)\n",
    "\n",
    "        return long_token\n",
    "\n",
    "    def long_terms_tokenizer(self, data_tokenize, target):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        target_list = []\n",
    "        \n",
    "        # get the input_ids and attention mask of the first 512 tokens\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        # get the input_ids of overflowing tokens\n",
    "        remain = data_tokenize['overflowing_tokens'].flatten()\n",
    "        target = torch.tensor(target, dtype=torch.int)\n",
    "\n",
    "        input_ids_list.append(previous_input_ids)\n",
    "        attention_mask_list.append(previous_attention_mask)\n",
    "        target_list.append(target)\n",
    "        \n",
    "        # segment the input_ids with overlapping\n",
    "        # some tricks are used here to segment with overlapping\n",
    "        if remain.shape[0] != 0:\n",
    "            idxs = range(len(remain)+self.chunk_len)\n",
    "            idxs = idxs[(self.chunk_len-self.overlap_len-2)::(self.chunk_len-self.overlap_len-2)]\n",
    "            input_ids_first_overlap = previous_input_ids[-(self.overlap_len+1):-1]\n",
    "            start_token = torch.tensor([101], dtype=torch.long)\n",
    "            end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "            for i, idx in enumerate(idxs):\n",
    "                if i == 0:\n",
    "                    input_ids = torch.cat(\n",
    "                        (input_ids_first_overlap, remain[:idx]))\n",
    "                elif i == len(idxs):\n",
    "                    input_ids = remain[idx:]\n",
    "                elif previous_idx >= len(remain):\n",
    "                    break\n",
    "                else:\n",
    "                    input_ids = remain[(previous_idx-self.overlap_len):idx]\n",
    "\n",
    "                previous_idx = idx\n",
    "\n",
    "                nb_token = len(input_ids)+2\n",
    "                attention_mask = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "                attention_mask[nb_token:self.chunk_len] = 0\n",
    "                input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "\n",
    "                if self.chunk_len-nb_token > 0:\n",
    "                    padding = torch.zeros(\n",
    "                        self.chunk_len-nb_token, dtype=torch.long)\n",
    "                    input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "                target_list.append(target)\n",
    "\n",
    "        return({\n",
    "            # input_ids_list [tensor seg1, tensor seg2, tensor seg3, ...]\n",
    "            # torch.tensor(ids, dtype=torch.long)\n",
    "            'input_ids': input_ids_list,  \n",
    "            'attention_mask': attention_mask_list,\n",
    "            # [tensor seg1, tensor seg2, tensor seg3,...]\n",
    "            'targets': target_list,\n",
    "            'len': [torch.tensor(len(target_list), dtype=torch.long)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    # return batches\n",
    "    # avoid shape error\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(newsgroups, tokenizer, max_len, batch_size):\n",
    "    ds = LongNewsDataset(\n",
    "        docs=newsgroups['data'],\n",
    "        #docs=newsgroups.data,\n",
    "        targets=newsgroups['target'],\n",
    "        #targets=newsgroups.target,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "      )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=my_collate1\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d313675",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "#BATCH_SIZE = 4\n",
    "MAX_LEN = 512\n",
    "\n",
    "train_data_loader = create_data_loader(train_ng, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_ng, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc285872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Hierarchical_Model(nn.Module):\n",
    "    def __init__(self, n_classes, pooling_method=\"mean\"):\n",
    "        super(BERT_Hierarchical_Model, self).__init__()\n",
    "        self.pooling_method = pooling_method\n",
    "        self.bert = BertModel.from_pretrained(BERTMODEL) #bert model from huggingface\n",
    "        self.drop = nn.Dropout(p=0.3) # add dropout of 0.3 on top of bert output\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # Linear layer as a classifier\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, lengt):\n",
    "        \n",
    "        # for example, when batch_size=4, four document, doc 1, doc 2, doc 4 are less than 512 tokens, doc 3 about 4*512 tokens\n",
    "        # input_ids:  tensor([[ doc 1], [doc 2], [doc 3 seg 1], [ doc 3 seg 2], [doc 3 seg 3], [doc 3 seg 4], [doc 4]])\n",
    "        \n",
    "        # pooled output: CLS token \n",
    "        # pooled_output shape: (number of segments of all documents in one batch * 786)\n",
    "        # In this small example, 7 * 786\n",
    "        # here not batch_size * 786 because of segmentation of long document\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        # split according to the number of segments \n",
    "        # then know which document the pooled_output belongs to \n",
    "        # chunks_emb (tensor([[doc1]]), tensor([[doc2]]), tensor([[doc3 seg1], [doc3 seg2], [doc3 seg3], [doc4 seg4]]), tensor([[doc4]])\n",
    "        chunks_emb = pooled_output.split_with_sizes(lengt) \n",
    "        \n",
    "        if self.pooling_method == \"mean\":\n",
    "            emb_pool = torch.stack([torch.mean(x, dim=0) for x in chunks_emb]) \n",
    "        elif self.pooling_method == \"max\":\n",
    "            # torch.max return (value, indice)\n",
    "            # here [0] to get the value\n",
    "            emb_pool = torch.stack([torch.max(x, dim=0)[0] for x in chunks_emb])\n",
    "        \n",
    "        output = self.out(self.drop(emb_pool))\n",
    "        #output = self.out(emb_pool)\n",
    "        return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Hierarchical_Model(len(train_ng['target']))\n",
    "#model =  BERT_Hierarchical_Model(len(train_ng.target_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "#EPOCHS = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        # for example, when batch_size=4, four document, doc 1, doc 2, doc 4 are less than 512 tokens, doc 3 about 4*512 tokens\n",
    "        # the input_ids would be:\n",
    "        # [tensor([[doc1 input_ids]]),tensor([[doc2 segment1 input_ids], [doc2 seg2], [doc2 seg3],[doc2 seg4]]), tensor([[doc3]]), tensor([[doc4]])]\n",
    "        input_ids = [data[\"input_ids\"] for data in batch]\n",
    "        # similar to input_ids\n",
    "        attention_mask = [data[\"attention_mask\"] for data in batch]\n",
    "        # [tensor(target), tensor(target), tensor(target), tensor(target)]  \n",
    "        # here [0] to reduce the dimension\n",
    "        targets = [data[\"targets\"][0] for data in batch]\n",
    "        # get the number of segments for each document \n",
    "        # [tensor([1]), tensor([1]), tensor([4]), tensor([1])]\n",
    "        lengt = [data['len'] for data in batch]\n",
    "        \n",
    "        # change the shape as input to Bert Model\n",
    "        # tensor([[ doc 1], [doc 2], [doc 3 seg 1], [ doc 3 seg 2], [doc 3 seg 3], [doc 3 seg 4], [doc 4]])\n",
    "        input_ids = torch.cat(input_ids)\n",
    "        attention_mask = torch.cat(attention_mask)\n",
    "        # tensor([doc1 target, doc2 target, doc3 target, doc4 target], dtype=torch.int32)\n",
    "        targets = torch.stack(targets)\n",
    "        # [doc1 num of segments, doc2 num of segments,... ]\n",
    "        # [1, 1, 4, 1]\n",
    "        lengt = [x.item() for x in lengt]\n",
    "        \n",
    "        input_ids = input_ids.to(device, dtype=torch.long)\n",
    "        attention_mask = attention_mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          lengt=lengt\n",
    "        )\n",
    "        \n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(float(loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"time = {time.time()-t0:.2f} secondes\")\n",
    "    t0 = time.time()\n",
    "        \n",
    "    return np.mean(losses), float(correct_predictions / n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            input_ids = [data[\"input_ids\"] for data in batch]\n",
    "            attention_mask = [data[\"attention_mask\"] for data in batch]\n",
    "            targets = [data[\"targets\"][0] for data in batch]\n",
    "            lengt = [data['len'] for data in batch]\n",
    "\n",
    "            input_ids = torch.cat(input_ids)\n",
    "            attention_mask = torch.cat(attention_mask)\n",
    "            targets = torch.stack(targets)\n",
    "            lengt = [x.item() for x in lengt]\n",
    "\n",
    "            input_ids = input_ids.to(device, dtype=torch.long)\n",
    "            attention_mask = attention_mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              lengt=lengt\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(float(loss.item()))\n",
    "\n",
    "    return np.mean(losses), float(correct_predictions / n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model,\n",
    "                                        train_data_loader,    \n",
    "                                        loss_fn, \n",
    "                                        optimizer, \n",
    "                                        device, \n",
    "                                        scheduler, \n",
    "                                        len(train_ng['data'])\n",
    "                                        #len(train_ng.data)\n",
    "                                        )\n",
    "\n",
    "    \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_loss, val_acc = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(val_ng['data'])\n",
    "    #len(val_ng.data)\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(history)\n",
    "    \n",
    "    #if val_acc > best_accuracy:\n",
    "        #torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        #best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig('BERT_Hierarchical_Model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
