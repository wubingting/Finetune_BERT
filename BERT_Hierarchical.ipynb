{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d72e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "#import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d4715",
   "metadata": {},
   "source": [
    "### BERT_Hierarchical\n",
    "\n",
    "The code is to fine-tune BERT by computing the pooled result from the output of each segment of the long document.\n",
    "It first uses BertTokenizer to encode the document, \n",
    "then get the encoding of the first 512 tokens and also the overflowing tokens if the document has more than the defined max length tokens.\n",
    "Based on the encoding of the first 512 tokens and the overflowing tokens (ids), we can get the encoding of the whole document and then segment\n",
    "the encoding with overlapping tokens. The encoding of each segment is then as input to the Bert model.\n",
    "For each segment we will get an output and then do pooling.\n",
    "\n",
    "The code is debugged step by step and some comments are added with small examples.\n",
    "\n",
    "The main code is extracted from https://github.com/helmy-elrais/RoBERT_Recurrence_over_BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ce127",
   "metadata": {},
   "source": [
    "### Data Preparing\n",
    "\n",
    "get the 20newsgroups dataset and then split into training set (90%) and validation set(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50da270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "size of training set: 10182\n",
      "size of validation set: 1132\n",
      "classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = [ \"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\",]\n",
    "\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, \n",
    "                                        random_state=238, remove=remove)\n",
    "# val_ng = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, \n",
    "#                                        random_state=42, remove=remove)\n",
    "\n",
    "# newsgroups = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "#                                       random_state=238, remove=remove)\n",
    "# test_ng = fetch_20newsgroups(subset='test', shuffle=True, \n",
    "#                                       random_state=238, remove=remove)\n",
    "\n",
    "data_train, data_val, label_train, label_val = train_test_split(newsgroups.data, newsgroups.target, test_size=0.1, random_state=42)\n",
    "train_ng = {'data': data_train, 'target': label_train}\n",
    "val_ng = {'data': data_val, 'target': label_val}\n",
    "\n",
    "print(\"data loaded\")\n",
    "\n",
    "# print('size of training set:', len(train_ng.data))\n",
    "# print('size of validation set:', len(val_ng.data))\n",
    "# print('classes:', train_ng.target_names)\n",
    "print('size of training set:', len(data_train))\n",
    "print('size of validation set:', len(data_val))\n",
    "print('classes:', newsgroups.target_names)\n",
    "\n",
    "# data_train = train_ng.data\n",
    "# label_train = train_ng.target\n",
    "# data_test = test_ng.data\n",
    "# label_test = test_ng.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04121ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#length = [len(l) for l in train_ng.data]\n",
    "length = [len(l) for l in train_ng['data']]\n",
    "plt.hist(length, bins=1000)\n",
    "plt.title(\"Frequency Histogram\")\n",
    "plt.xlabel(\"length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim((0, 5000))\n",
    "plt.xticks(np.arange(0, 5000, step=500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d15b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BertTokenizer\n"
     ]
    }
   ],
   "source": [
    "print('Initializing BertTokenizer')\n",
    "\n",
    "BERTMODEL='bert-base-uncased'\n",
    "CACHE_DIR='transformers-cache'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL, cache_dir=CACHE_DIR,\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4180b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda:2\") # specify  devicethe\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a563dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input_ids_list, attention_mask_list of the long document. Each element in the list correspondes to a segment.\n",
    "# also get target for each segment and the number of segments.\n",
    "class LongNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, docs, targets, tokenizer, max_len, chunk_len=512, overlap_len=50):\n",
    "        self.docs = docs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        doc = str(self.docs[item])\n",
    "        target = int(self.targets[item])\n",
    "        \n",
    "        # get the encoding of the first 512 tokens and the overflowing tokens if the document has more than the defined max length tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          doc,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.chunk_len,\n",
    "          truncation=True,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_overflowing_tokens=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        long_token = self.long_terms_tokenizer(encoding, target)\n",
    "\n",
    "        return long_token\n",
    "\n",
    "    def long_terms_tokenizer(self, data_tokenize, target):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        target_list = []\n",
    "        \n",
    "        # get the input_ids and attention mask of the first 512 tokens\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        # get the input_ids of overflowing tokens\n",
    "        remain = data_tokenize['overflowing_tokens'].flatten()\n",
    "        target = torch.tensor(target, dtype=torch.int)\n",
    "\n",
    "        input_ids_list.append(previous_input_ids)\n",
    "        attention_mask_list.append(previous_attention_mask)\n",
    "        target_list.append(target)\n",
    "        \n",
    "        # segment the input_ids with overlapping\n",
    "        # some tricks are used here to segment with overlapping\n",
    "        if remain.shape[0] != 0:\n",
    "            idxs = range(len(remain)+self.chunk_len)\n",
    "            idxs = idxs[(self.chunk_len-self.overlap_len-2)::(self.chunk_len-self.overlap_len-2)]\n",
    "            input_ids_first_overlap = previous_input_ids[-(self.overlap_len+1):-1]\n",
    "            start_token = torch.tensor([101], dtype=torch.long)\n",
    "            end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "            for i, idx in enumerate(idxs):\n",
    "                if i == 0:\n",
    "                    input_ids = torch.cat(\n",
    "                        (input_ids_first_overlap, remain[:idx]))\n",
    "                elif i == len(idxs):\n",
    "                    input_ids = remain[idx:]\n",
    "                elif previous_idx >= len(remain):\n",
    "                    break\n",
    "                else:\n",
    "                    input_ids = remain[(previous_idx-self.overlap_len):idx]\n",
    "\n",
    "                previous_idx = idx\n",
    "\n",
    "                nb_token = len(input_ids)+2\n",
    "                attention_mask = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "                attention_mask[nb_token:self.chunk_len] = 0\n",
    "                input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "\n",
    "                if self.chunk_len-nb_token > 0:\n",
    "                    padding = torch.zeros(\n",
    "                        self.chunk_len-nb_token, dtype=torch.long)\n",
    "                    input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "                target_list.append(target)\n",
    "\n",
    "        return({\n",
    "            # input_ids_list [tensor seg1, tensor seg2, tensor seg3, ...]\n",
    "            # torch.tensor(ids, dtype=torch.long)\n",
    "            'input_ids': input_ids_list,  \n",
    "            'attention_mask': attention_mask_list,\n",
    "            # [tensor seg1, tensor seg2, tensor seg3,...]\n",
    "            'targets': target_list,\n",
    "            'len': [torch.tensor(len(target_list), dtype=torch.long)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df9549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    # return batches\n",
    "    # avoid shape error\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc1f4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(newsgroups, tokenizer, max_len, batch_size):\n",
    "    ds = LongNewsDataset(\n",
    "        docs=newsgroups['data'],\n",
    "        #docs=newsgroups.data,\n",
    "        targets=newsgroups['target'],\n",
    "        #targets=newsgroups.target,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "      )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=my_collate1\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d313675",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "#BATCH_SIZE = 4\n",
    "MAX_LEN = 512\n",
    "\n",
    "train_data_loader = create_data_loader(train_ng, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_ng, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc285872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Hierarchical_Model(nn.Module):\n",
    "    def __init__(self, n_classes, pooling_method=\"mean\"):\n",
    "        super(BERT_Hierarchical_Model, self).__init__()\n",
    "        self.pooling_method = pooling_method\n",
    "        self.bert = BertModel.from_pretrained(BERTMODEL) #bert model from huggingface\n",
    "        self.drop = nn.Dropout(p=0.3) # add dropout of 0.3 on top of bert output\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # Linear layer as a classifier\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, lengt):\n",
    "        \n",
    "        # for example, when batch_size=4, four document, doc 1, doc 2, doc 4 are less than 512 tokens, doc 3 about 4*512 tokens\n",
    "        # input_ids:  tensor([[ doc 1], [doc 2], [doc 3 seg 1], [ doc 3 seg 2], [doc 3 seg 3], [doc 3 seg 4], [doc 4]])\n",
    "        \n",
    "        # pooled output: CLS token \n",
    "        # pooled_output shape: (number of segments of all documents in one batch * 786)\n",
    "        # In this small example, 7 * 786\n",
    "        # here not batch_size * 786 because of segmentation of long document\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        # split according to the number of segments \n",
    "        # then know which document the pooled_output belongs to \n",
    "        # chunks_emb (tensor([[doc1]]), tensor([[doc2]]), tensor([[doc3 seg1], [doc3 seg2], [doc3 seg3], [doc4 seg4]]), tensor([[doc4]])\n",
    "        chunks_emb = pooled_output.split_with_sizes(lengt) \n",
    "        \n",
    "        if self.pooling_method == \"mean\":\n",
    "            emb_pool = torch.stack([torch.mean(x, dim=0) for x in chunks_emb]) \n",
    "        elif self.pooling_method == \"max\":\n",
    "            # torch.max return (value, indice)\n",
    "            # here [0] to get the value\n",
    "            emb_pool = torch.stack([torch.max(x, dim=0)[0] for x in chunks_emb])\n",
    "        \n",
    "        output = self.out(self.drop(emb_pool))\n",
    "        #output = self.out(emb_pool)\n",
    "        return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6583922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERT_Hierarchical_Model(len(train_ng['target']))\n",
    "#model =  BERT_Hierarchical_Model(len(train_ng.target_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee0b425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "#EPOCHS = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04a9ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        # for example, when batch_size=4, four document, doc 1, doc 2, doc 4 are less than 512 tokens, doc 3 about 4*512 tokens\n",
    "        # the input_ids would be:\n",
    "        # [tensor([[doc1 input_ids]]),tensor([[doc2 segment1 input_ids], [doc2 seg2], [doc2 seg3],[doc2 seg4]]), tensor([[doc3]]), tensor([[doc4]])]\n",
    "        input_ids = [data[\"input_ids\"] for data in batch]\n",
    "        # similar to input_ids\n",
    "        attention_mask = [data[\"attention_mask\"] for data in batch]\n",
    "        # [tensor(target), tensor(target), tensor(target), tensor(target)]  \n",
    "        # here [0] to reduce the dimension\n",
    "        targets = [data[\"targets\"][0] for data in batch]\n",
    "        # get the number of segments for each document \n",
    "        # [tensor([1]), tensor([1]), tensor([4]), tensor([1])]\n",
    "        lengt = [data['len'] for data in batch]\n",
    "        \n",
    "        # change the shape as input to Bert Model\n",
    "        # tensor([[ doc 1], [doc 2], [doc 3 seg 1], [ doc 3 seg 2], [doc 3 seg 3], [doc 3 seg 4], [doc 4]])\n",
    "        input_ids = torch.cat(input_ids)\n",
    "        attention_mask = torch.cat(attention_mask)\n",
    "        # tensor([doc1 target, doc2 target, doc3 target, doc4 target], dtype=torch.int32)\n",
    "        targets = torch.stack(targets)\n",
    "        # [doc1 num of segments, doc2 num of segments,... ]\n",
    "        # [1, 1, 4, 1]\n",
    "        lengt = [x.item() for x in lengt]\n",
    "        \n",
    "        input_ids = input_ids.to(device, dtype=torch.long)\n",
    "        attention_mask = attention_mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          lengt=lengt\n",
    "        )\n",
    "        \n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(float(loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"time = {time.time()-t0:.2f} secondes\")\n",
    "    t0 = time.time()\n",
    "        \n",
    "    return np.mean(losses), float(correct_predictions / n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ff2d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            input_ids = [data[\"input_ids\"] for data in batch]\n",
    "            attention_mask = [data[\"attention_mask\"] for data in batch]\n",
    "            targets = [data[\"targets\"][0] for data in batch]\n",
    "            lengt = [data['len'] for data in batch]\n",
    "\n",
    "            input_ids = torch.cat(input_ids)\n",
    "            attention_mask = torch.cat(attention_mask)\n",
    "            targets = torch.stack(targets)\n",
    "            lengt = [x.item() for x in lengt]\n",
    "\n",
    "            input_ids = input_ids.to(device, dtype=torch.long)\n",
    "            attention_mask = attention_mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              lengt=lengt\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(float(loss.item()))\n",
    "\n",
    "    return np.mean(losses), float(correct_predictions / n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59a0e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WUBING~1\\AppData\\Local\\Temp/ipykernel_93668/1622122143.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     train_loss, train_acc = train_epoch(model,\n\u001b[0m\u001b[0;32m     10\u001b[0m                                         \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                         \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WUBING~1\\AppData\\Local\\Temp/ipykernel_93668/306188303.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model,\n",
    "                                        train_data_loader,    \n",
    "                                        loss_fn, \n",
    "                                        optimizer, \n",
    "                                        device, \n",
    "                                        scheduler, \n",
    "                                        len(train_ng['data'])\n",
    "                                        #len(train_ng.data)\n",
    "                                        )\n",
    "\n",
    "    \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_loss, val_acc = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(val_ng['data'])\n",
    "    #len(val_ng.data)\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(history)\n",
    "    \n",
    "    #if val_acc > best_accuracy:\n",
    "        #torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        #best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bacd8305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig('BERT_Hierarchical_Model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
