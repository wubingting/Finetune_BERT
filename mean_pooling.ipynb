{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccbd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e52a0",
   "metadata": {},
   "source": [
    "The code is to fine-tune BERT by computing the pooled result from the output of each segment of the long document.\n",
    "\n",
    "It first uses BertTokenizer to encode the document, \n",
    "then get the encoding of the first 512 tokens and also the overflowing tokens if the document has more than the defined max length tokens.\n",
    "Based on the encoding of the first 512 tokens and the overflowing tokens (ids), we can get the encoding of the whole document and then segment\n",
    "the encoding with overlapping tokens. The encoding of each segment is then as input to the Bert model.\n",
    "For each segment we will get an output and then do pooling.\n",
    "\n",
    "The main code is extracted from https://github.com/helmy-elrais/RoBERT_Recurrence_over_BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e91f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 10182\n",
      "size of training set: 10182\n",
      "size of validation set: 1132\n",
      "size of validation set: 1132\n",
      "classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# get the 20newsgroups dataset and then split into training set (90%) and validation set(10%)\n",
    "\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "newsgroups = fetch_20newsgroups(subset='train', shuffle=True,\n",
    "                                random_state=238, remove=remove)\n",
    "data_train, data_val, label_train, label_val = train_test_split(newsgroups.data, newsgroups.target, test_size=0.1,\n",
    "                                                                random_state=42)\n",
    "train_ng = {'data': data_train, 'target': label_train}\n",
    "val_ng = {'data': data_val, 'target': label_val}\n",
    "\n",
    "\n",
    "print('size of training set:', len(data_train))\n",
    "print('size of training set:', len(label_train))\n",
    "print('size of validation set:', len(data_val))\n",
    "print('size of validation set:', len(label_val))\n",
    "print('classes:', newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752685e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BertTokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2156b632d04c33bec4028d2262f64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3627f23b2d364a7e9b1cdfac8c24833b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6d07d2da7448e8bd0f7627f986aaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Initializing BertTokenizer')\n",
    "\n",
    "BERTMODEL = 'bert-base-uncased'\n",
    "CACHE_DIR = '../transformers-cache'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL, cache_dir=CACHE_DIR,\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab12f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:2\")  # specify  devicethe\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f707594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input_ids_list, attention_mask_list of the long document. Each element in the list correspondes to a segment.\n",
    "# also get target for each segment and the number of segments.\n",
    "\n",
    "class LongNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, docs, targets, tokenizer, max_len, chunk_len=512, overlap_len=50):\n",
    "        self.docs = docs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        doc = str(self.docs[item])\n",
    "        target = int(self.targets[item])\n",
    "\n",
    "        # get the encoding of the first 512 tokens and the overflowing tokens if the document has more than the defined max length tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            doc,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        long_token = self.long_terms_tokenizer(encoding, target)\n",
    "\n",
    "        return long_token\n",
    "\n",
    "    def long_terms_tokenizer(self, data_tokenize, target):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        target_list = []\n",
    "\n",
    "        # get the input_ids and attention mask of the first 512 tokens\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        # get the input_ids of overflowing tokens\n",
    "        remain = data_tokenize['overflowing_tokens'].flatten()\n",
    "        target = torch.tensor(target, dtype=torch.int)\n",
    "\n",
    "        input_ids_list.append(previous_input_ids)\n",
    "        attention_mask_list.append(previous_attention_mask)\n",
    "        target_list.append(target)\n",
    "\n",
    "        # segment the input_ids with overlapping\n",
    "        # some tricks are used here to segment with overlapping\n",
    "        if remain.shape[0] != 0:\n",
    "            idxs = range(len(remain) + self.chunk_len)\n",
    "            idxs = idxs[(self.chunk_len - self.overlap_len - 2)::(self.chunk_len - self.overlap_len - 2)]\n",
    "            input_ids_first_overlap = previous_input_ids[-(self.overlap_len + 1):-1]\n",
    "            start_token = torch.tensor([101], dtype=torch.long)\n",
    "            end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "            for i, idx in enumerate(idxs):\n",
    "                if i == 0:\n",
    "                    input_ids = torch.cat(\n",
    "                        (input_ids_first_overlap, remain[:idx]))\n",
    "                elif i == len(idxs):\n",
    "                    input_ids = remain[idx:]\n",
    "                elif previous_idx >= len(remain):\n",
    "                    break\n",
    "                else:\n",
    "                    input_ids = remain[(previous_idx - self.overlap_len):idx]\n",
    "\n",
    "                previous_idx = idx\n",
    "\n",
    "                nb_token = len(input_ids) + 2\n",
    "                attention_mask = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "                attention_mask[nb_token:self.chunk_len] = 0\n",
    "                input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "\n",
    "                if self.chunk_len - nb_token > 0:\n",
    "                    padding = torch.zeros(\n",
    "                        self.chunk_len - nb_token, dtype=torch.long)\n",
    "                    input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "                target_list.append(target)\n",
    "\n",
    "        # input_ids_list[:60] if len(input_ids_list) > 60 else input_ids_list\n",
    "        # attention_mask_list[:60] if len(attention_mask_list) > 60 else attention_mask_list\n",
    "\n",
    "        return ({\n",
    "            # input_ids_list [tensor seg1, tensor seg2, tensor seg3, ...]\n",
    "            # torch.tensor(ids, dtype=torch.long)\n",
    "            'input_ids': input_ids_list,\n",
    "            'attention_mask': attention_mask_list,\n",
    "            # [tensor seg1, tensor seg2, tensor seg3,...]\n",
    "            'targets': target_list,\n",
    "            'len': [torch.tensor(len(target_list), dtype=torch.long)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716b2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    # return batches\n",
    "    # avoid shape error\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4ac507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(newsgroups, tokenizer, max_len, batch_size):\n",
    "    ds = LongNewsDataset(\n",
    "        docs=newsgroups['data'],\n",
    "        targets=newsgroups['target'],\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=my_collate1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587188e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "# max len of each segment\n",
    "MAX_LEN = 512\n",
    "\n",
    "train_data_loader = create_data_loader(train_ng, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_ng, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f239f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Hierarchical_Model(nn.Module):\n",
    "    def __init__(self, n_classes, pooling_method=\"mean\"):\n",
    "        super(BERT_Hierarchical_Model, self).__init__()\n",
    "        self.pooling_method = pooling_method\n",
    "        self.bert = BertModel.from_pretrained(BERTMODEL)  # bert model from huggingface\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)  # Linear layer as a classifier\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, lengt):\n",
    "\n",
    "        # for example, when batch_size=4, four document, doc 1, doc 2, doc 4 are less than 512 tokens, doc 3 about 4*512 tokens\n",
    "        # input_ids:  tensor([[ doc 1], [doc 2], [doc 3 seg 1], [ doc 3 seg 2], [doc 3 seg 3], [doc 3 seg 4], [doc 4]])\n",
    "\n",
    "        # pooled output: CLS token \n",
    "        # pooled_output shape: (number of segments of all documents in one batch * 786)\n",
    "        # In this small example, 7 * 786\n",
    "        # here not batch_size * 786 because of segmentation of long document\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "\n",
    "        # split according to the number of segments \n",
    "        # then know which document the pooled_output belongs to \n",
    "        # chunks_emb (tensor([[doc1]]), tensor([[doc2]]), tensor([[doc3 seg1], [doc3 seg2], [doc3 seg3], [doc4 seg4]]), tensor([[doc4]])\n",
    "        chunks_emb = pooled_output.split_with_sizes(lengt)\n",
    "\n",
    "        if self.pooling_method == \"mean\":\n",
    "            emb_pool = torch.stack([torch.mean(x, dim=0) for x in chunks_emb])\n",
    "        elif self.pooling_method == \"max\":\n",
    "            # torch.max return (value, indice)\n",
    "            # here [0] to get the value\n",
    "            emb_pool = torch.stack([torch.max(x, dim=0)[0] for x in chunks_emb])\n",
    "\n",
    "        output = self.out(emb_pool)\n",
    "        return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cac4a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERT_Hierarchical_Model(len(set(train_ng['target'])))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af11fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f722620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # for example, when batch_size=4, four document, doc 1, doc 2, doc 4 are less than 512 tokens, doc 3 about 4*512 tokens\n",
    "        # the input_ids would be:\n",
    "        # [tensor([[doc1 input_ids]]),tensor([[doc2 segment1 input_ids], [doc2 seg2], [doc2 seg3],[doc2 seg4]]), tensor([[doc3]]), tensor([[doc4]])]\n",
    "        input_ids = [data[\"input_ids\"] for data in batch]\n",
    "        # similar to input_ids\n",
    "        attention_mask = [data[\"attention_mask\"] for data in batch]\n",
    "        # [tensor(target), tensor(target), tensor(target), tensor(target)]  \n",
    "        # here [0] to reduce the dimension\n",
    "        targets = [data[\"targets\"][0] for data in batch]\n",
    "        # get the number of segments for each document \n",
    "        # [tensor([1]), tensor([1]), tensor([4]), tensor([1])]\n",
    "        lengt = [data['len'] for data in batch]\n",
    "\n",
    "        # change the shape as input to Bert Model\n",
    "        # tensor([[ doc 1], [doc 2], [doc 3 seg 1], [ doc 3 seg 2], [doc 3 seg 3], [doc 3 seg 4], [doc 4]])\n",
    "        input_ids = torch.cat(input_ids)\n",
    "        attention_mask = torch.cat(attention_mask)\n",
    "        # tensor([doc1 target, doc2 target, doc3 target, doc4 target], dtype=torch.int32)\n",
    "        targets = torch.stack(targets)\n",
    "        # [doc1 num of segments, doc2 num of segments,... ]\n",
    "        # [1, 1, 4, 1]\n",
    "        lengt = [x.item() for x in lengt]\n",
    "\n",
    "        input_ids = input_ids.to(device, dtype=torch.long)\n",
    "        attention_mask = attention_mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            lengt=lengt\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(float(loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"time = {time.time() - t0:.2f} secondes\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    return np.mean(losses), float(correct_predictions / n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a53a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids = [data[\"input_ids\"] for data in batch]\n",
    "            attention_mask = [data[\"attention_mask\"] for data in batch]\n",
    "            targets = [data[\"targets\"][0] for data in batch]\n",
    "            lengt = [data['len'] for data in batch]\n",
    "\n",
    "            input_ids = torch.cat(input_ids)\n",
    "            attention_mask = torch.cat(attention_mask)\n",
    "            targets = torch.stack(targets)\n",
    "            lengt = [x.item() for x in lengt]\n",
    "\n",
    "            input_ids = input_ids.to(device, dtype=torch.long)\n",
    "            attention_mask = attention_mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                lengt=lengt\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(float(loss.item()))\n",
    "\n",
    "    return np.mean(losses), float(correct_predictions / n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa8c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WUBING~1\\AppData\\Local\\Temp/ipykernel_31148/3121098990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     train_loss, train_acc = train_epoch(model,\n\u001b[0m\u001b[0;32m     10\u001b[0m                                         \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                         \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WUBING~1\\AppData\\Local\\Temp/ipykernel_31148/1160709468.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         outputs = model(\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WUBING~1\\AppData\\Local\\Temp/ipykernel_31148/353035203.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, lengt)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# In this small example, 7 * 786\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# here not batch_size * 786 because of segmentation of long document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         _, pooled_output = self.bert(\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 )\n\u001b[0;32m    584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 402\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wubingting\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1816\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1818\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1819\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model,\n",
    "                                        train_data_loader,\n",
    "                                        loss_fn,\n",
    "                                        optimizer,\n",
    "                                        device,\n",
    "                                        scheduler,\n",
    "                                        len(train_ng['data'])\n",
    "                                        )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_loss, val_acc = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(val_ng['data'])\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1778f126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax.set_xticks(np.arange(0, 50, 5))\n",
    "\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('mean_pooling')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('mean_pooling.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

